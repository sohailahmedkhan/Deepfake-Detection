{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c38dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from functools import reduce\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToPILImage\n",
    "# from linformer import Linformer\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "# from vit_pytorch.efficient import ViT\n",
    "# from model import BiSeNet\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io, img_as_float\n",
    "import timm\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62df40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6fa774",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeavceleb_paths_df = pd.read_csv('File_Path_Dictionaries/fakeavceleb_paths.csv')\n",
    "celebdf_paths_df = pd.read_csv('File_Path_Dictionaries/celebdf_paths.csv')\n",
    "dfdc_paths_df = pd.read_csv('File_Path_Dictionaries/dfdc_paths.csv')\n",
    "faceswap_paths_df = pd.read_csv('File_Path_Dictionaries/ff_paths.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed5253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_paths_dictionary = {\n",
    "                            'FakeAVCeleb': [fakeavceleb_paths_df['Path'], fakeavceleb_paths_df['Label']], \n",
    "                            'DFDC': [dfdc_paths_df['Path'], dfdc_paths_df['Label']],\n",
    "                            'CelebDFV2': [celebdf_paths_df['Path'], celebdf_paths_df['Label']],\n",
    "                            'FaceForensics': [faceswap_paths_df['Path'], faceswap_paths_df['Label']]\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7547593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "models = ['xception', 'res2net101_26w_4s', 'tf_efficientnet_b7_ns', 'vit_base_patch16_224', 'swin_base_patch4_window7_224', 'mvitv2_base']\n",
    "augs = ['No_Augmentations', 'Random_Cut_Out_Augs', 'Face_Cut_Out_Augs']\n",
    "datasets = ['FakeAVCeleb', 'FaceForensics', 'DFDC', 'CelebDFV2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9500ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(predictions):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    for key, value in predictions.items():\n",
    "        predicted_probs.append(value[0])\n",
    "        true_labels.append(value[2])\n",
    "        predicted_labels.append(value[1])\n",
    "\n",
    "    logloss = log_loss(true_labels, predicted_probs)\n",
    "    auc = roc_auc_score(true_labels, predicted_probs)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return logloss, auc, f1\n",
    "\n",
    "def Evaluate(model, vids_path_dict, dataset, save_path):\n",
    "    \n",
    "    imgs_path = vids_path_dict[dataset][0]\n",
    "    count = 0\n",
    "    labels_map = [\"fake\", \"real\"]\n",
    "    predictions = {}\n",
    "    probs = {}\n",
    "    for img in imgs_path:\n",
    "\n",
    "        originallabel = None\n",
    "        if 'fake' in img.split('/')[-3]:\n",
    "            originallabel = 0\n",
    "        elif 'real' in img.split('/')[-3]:\n",
    "            originallabel = 1\n",
    "\n",
    "        count+=1\n",
    "        print(count)\n",
    "        image = img.replace('\\\\','/')\n",
    "        img = Image.open(image)\n",
    "        tfms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "               transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "        img = tfms(img)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img.unsqueeze(0).to(device))\n",
    "\n",
    "        predicted_prob = []\n",
    "        predicted_label = None\n",
    "        for idx in torch.topk(outputs[0], k=1).indices.tolist():\n",
    "            prob = torch.softmax(outputs[0], 0)[idx].item()\n",
    "\n",
    "        if labels_map[idx] == 'fake':\n",
    "            predicted_prob = 1 - prob\n",
    "            predictions[image] = [predicted_prob, 0, originallabel]\n",
    "        else:\n",
    "            predicted_prob = prob\n",
    "            print(predicted_prob)\n",
    "            predictions[image] = [predicted_prob, 1, originallabel]\n",
    "        print('--------------')\n",
    "        \n",
    "    if 'neuraltextures' in image:\n",
    "        originallabel = 'NeuralTextures'\n",
    "    elif 'face2face' in image:\n",
    "        originallabel = 'Face2Face'\n",
    "    elif 'deepfakes' in image:\n",
    "        originallabel = 'DeepFakes'\n",
    "    elif 'faceswap' in image:\n",
    "        originallabel = 'FaceSwap'\n",
    "        \n",
    "    with open(save_path + '.csv', 'w') as csvfile:\n",
    "        fieldnames = ['ImagePath', 'ProbabilityScore', 'PredictedLabel', 'OriginalLabel']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, lineterminator='\\n',)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for i in range(len(predictions)):\n",
    "            img = list(predictions)[i]\n",
    "            writer.writerow({'ImagePath': list(predictions)[i], 'ProbabilityScore': predictions[img][0], 'PredictedLabel': predictions[img][1], 'OriginalLabel': predictions[img][2]})\n",
    "    \n",
    "    \n",
    "    logloss, auc, f1 = get_scores(predictions)\n",
    "    fieldnames = ['LogLoss','AUC','F1-Score']\n",
    "    with open(save_path + '.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, lineterminator='\\n',)\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'LogLoss': logloss, 'AUC': auc, 'F1-Score': f1})\n",
    "    torch.cuda.empty_cache()\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea975d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    no_evaluation_on = datasets[i]\n",
    "#     print(\"NO EVALUATION DATASET IS: \", no_evaluation_on)\n",
    "    for model_name in models:\n",
    "        for dataset in datasets:\n",
    "#             if no_evaluation_on == dataset:\n",
    "            if 1 == 2:\n",
    "                pass\n",
    "            else:\n",
    "                # Load model weights\n",
    "                PATH = '../model_weights/supervised_models/' + model_name + '/' + augs[0] + '/' + no_evaluation_on + '/' + '/'\n",
    "                for file in os.listdir(PATH):\n",
    "                    if file.endswith(\".pth\"):\n",
    "                        model_path = os.path.join(PATH, file)\n",
    "                        model_path = model_path.replace(\"//\", \"/\")\n",
    "                        print(model_path)\n",
    "                        model = timm.create_model(model_name, pretrained=True, num_classes=2).to(device)\n",
    "                        model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                        model.to(device)\n",
    "                        model.eval()\n",
    "                        print(\"loaded \" + model_name + \" weights!\")\n",
    "                        \n",
    "                        if dataset == 'FakeAVCeleb':\n",
    "                            #Evaluation code\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "\n",
    "                        elif dataset == 'DFDC':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                            \n",
    "                        elif dataset == 'CelebDFV2':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                            \n",
    "                        elif dataset == 'FaceForensics':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                                \n",
    "                        print('====================================================================================================')\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d3dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    no_evaluation_on = datasets[i]\n",
    "#     print(\"NO EVALUATION DATASET IS: \", no_evaluation_on)\n",
    "    for model_name in models:\n",
    "        for dataset in datasets:\n",
    "#             if no_evaluation_on == dataset:\n",
    "            if 1 == 2:\n",
    "                pass\n",
    "            else:\n",
    "                # Load model weights\n",
    "                PATH = '../model_weights/supervised_models/' + model_name + '/' + augs[1] + '/' + no_evaluation_on + '/' + '/'\n",
    "                for file in os.listdir(PATH):\n",
    "                    if file.endswith(\".pth\"):\n",
    "                        model_path = os.path.join(PATH, file)\n",
    "                        model_path = model_path.replace(\"//\", \"/\")\n",
    "                        print(model_path)\n",
    "                        model = timm.create_model(model_name, pretrained=True, num_classes=2).to(device)\n",
    "                        model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                        model.to(device)\n",
    "                        model.eval()\n",
    "                        print(\"loaded \" + model_name + \" weights!\")\n",
    "                        \n",
    "                        if dataset == 'FakeAVCeleb':\n",
    "                            #Evaluation code\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            predictions = Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "\n",
    "                        elif dataset == 'DFDC':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                            \n",
    "                        elif dataset == 'CelebDFV2':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                            \n",
    "                        elif dataset == 'FaceForensics':\n",
    "                            print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + no_evaluation_on)\n",
    "                            save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "                                         no_evaluation_on + '_evaluated_on_' + dataset)\n",
    "                            Evaluate(model, test_set_paths_dictionary, dataset, save_path)\n",
    "                                \n",
    "                        print('====================================================================================================')\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "                        \n",
    "#     print('')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0af1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
