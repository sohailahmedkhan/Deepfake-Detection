{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c38dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from functools import reduce\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToPILImage\n",
    "# from linformer import Linformer\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "# from vit_pytorch.efficient import ViT\n",
    "# from model import BiSeNet\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io, img_as_float\n",
    "import timm\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225f5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TimeSformer.timesformer.models.vit import TimeSformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62df40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6fa774",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeavceleb_paths_df = pd.read_csv('File_Path_Dictionaries/fakeavceleb_paths.csv')\n",
    "celebdf_paths_df = pd.read_csv('File_Path_Dictionaries/celebdf_paths.csv')\n",
    "dfdc_paths_df = pd.read_csv('File_Path_Dictionaries/dfdc_paths.csv')\n",
    "faceswap_paths_df = pd.read_csv('File_Path_Dictionaries/ff_paths.csv')\n",
    "\n",
    "video_fakeavceleb_paths_df = pd.read_csv('Video_File_Path_Dictionaries/fakeavceleb_paths.csv')\n",
    "video_celebdf_paths_df = pd.read_csv('Video_File_Path_Dictionaries/celebdf_paths.csv')\n",
    "video_dfdc_paths_df = pd.read_csv('Video_File_Path_Dictionaries/dfdc_paths.csv')\n",
    "video_faceswap_paths_df = pd.read_csv('Video_File_Path_Dictionaries/ff_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed5253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_paths_dictionary = {\n",
    "                            'FakeAVCeleb': [fakeavceleb_paths_df['Path'], fakeavceleb_paths_df['Label']], \n",
    "                            'DFDC': [dfdc_paths_df['Path'], dfdc_paths_df['Label']],\n",
    "                            'CelebDFV2': [celebdf_paths_df['Path'], celebdf_paths_df['Label']],\n",
    "                            'FaceForensics': [faceswap_paths_df['Path'], faceswap_paths_df['Label']]\n",
    "                            }\n",
    "\n",
    "video_test_set_paths_dictionary = {\n",
    "                            'FakeAVCeleb': [video_fakeavceleb_paths_df['Path'], video_fakeavceleb_paths_df['Label']], \n",
    "                            'DFDC': [video_dfdc_paths_df['Path'], video_dfdc_paths_df['Label']],\n",
    "                            'CelebDFV2': [video_celebdf_paths_df['Path'], video_celebdf_paths_df['Label']],\n",
    "                            'FaceForensics': [video_faceswap_paths_df['Path'], video_faceswap_paths_df['Label']]\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7547593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "models = ['swin_base_patch4_window7_224', 'xception', 'res2net101_26w_4s', 'tf_efficientnet_b7_ns', 'vit_base_patch16_224', \n",
    "          'mvitv2_base', 'ResNet', 'TimeSformer']\n",
    "augs = ['No_Augmentations', 'Random_Cut_Out_Augs', 'Face_Cut_Out_Augs']\n",
    "datasets = ['FakeAVCeleb', 'FaceForensics', 'DFDC', 'CelebDFV2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9500ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(model, vids_path_dict, dataset):\n",
    "    \n",
    "    imgs_path = vids_path_dict[dataset][0]\n",
    "    count = 0\n",
    "    labels_map = [\"fake\", \"real\"]\n",
    "    predictions = {}\n",
    "    probs = {}\n",
    "    for img in imgs_path:\n",
    "\n",
    "        originallabel = None\n",
    "        if 'fake' in img.split('/')[-3]:\n",
    "            originallabel = 0\n",
    "        elif 'real' in img.split('/')[-3]:\n",
    "            originallabel = 1\n",
    "\n",
    "        count+=1\n",
    "        print(count)\n",
    "        image = img.replace('\\\\','/')\n",
    "        img = Image.open(image)\n",
    "        tfms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "               transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "        img = tfms(img)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img.unsqueeze(0).to(device))\n",
    "\n",
    "        predicted_prob = []\n",
    "        predicted_label = None\n",
    "        for idx in torch.topk(outputs[0], k=1).indices.tolist():\n",
    "            prob = torch.softmax(outputs[0], 0)[idx].item()\n",
    "\n",
    "        if labels_map[idx] == 'fake':\n",
    "            predicted_prob = 1 - prob\n",
    "            predictions[image] = [predicted_prob, 0, originallabel]\n",
    "        else:\n",
    "            predicted_prob = prob\n",
    "            print(predicted_prob)\n",
    "            predictions[image] = [predicted_prob, 1, originallabel]\n",
    "        print('--------------')\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions\n",
    "\n",
    "def Video_Evaluate(model, vids_path_dict, dataset):\n",
    "    videos = vids_path_dict[dataset][0]\n",
    "    count = 0\n",
    "    labels_map = [\"fake\", \"real\"]\n",
    "    predictions = {}\n",
    "    probs = {}\n",
    "    tfms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    for vid in videos:\n",
    "        fake_prob = 0\n",
    "        real_prob = 0\n",
    "        originallabel = None\n",
    "        if 'fake' in vid.split('/')[-2]:\n",
    "            originallabel = 0\n",
    "        elif 'real' in vid.split('/')[-2]:\n",
    "            originallabel = 1\n",
    "        count+=1\n",
    "        print(count)\n",
    "        images = sorted(glob.glob(vid + '/*.png'))\n",
    "        images = [path.replace('\\\\','/') for path in images]\n",
    "        concated_images_per_video = torch.zeros(1, 3, 224, 224)\n",
    "        for i in range(8):\n",
    "            img_path = images[i]\n",
    "            img = Image.open(img_path)\n",
    "            img_transformed = tfms(img)\n",
    "            img_transformed = img_transformed.unsqueeze(0)\n",
    "            concated_images_per_video = torch.cat((concated_images_per_video, img_transformed), dim=0)\n",
    "        concated_images_per_video = concated_images_per_video[1:]\n",
    "        concated_images_per_video = concated_images_per_video.permute(1, 0, 2, 3)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(concated_images_per_video.unsqueeze(0).to('cuda'))\n",
    "        predicted_prob = []\n",
    "        predicted_label = None\n",
    "        for idx in torch.topk(outputs[0], k=1).indices.tolist():\n",
    "            prob = torch.softmax(outputs[0], 0)[idx].item()\n",
    "        if labels_map[idx] == 'fake':\n",
    "            predicted_prob = 1 - prob\n",
    "            print(predicted_prob)\n",
    "            predictions[vid] = [predicted_prob, 0, originallabel]\n",
    "        else:\n",
    "            predicted_prob = prob\n",
    "            print(predicted_prob)\n",
    "            predictions[vid] = [predicted_prob, 1, originallabel]\n",
    "        print('--------------')\n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquarel import load_theme, Theme\n",
    "plot_colors = [\"#fc358f\", \"#EEA47FFF\", '#101820FF', '#FEE715FF', '#AA96DA', '#317773', '#990011FF', '#00FFFF']\n",
    "theme = (\n",
    "    Theme(name=\"scientific\")\n",
    "    .set_grid(draw=True, width=0.5)\n",
    "#     .set_font(family=\"monospace\")\n",
    "    .set_color(palette=plot_colors, plot_background_color=\"#FFFDFB\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea975d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for model_name in models:\n",
    "        # Load model weights\n",
    "        PATH = '../model_weights/supervised_models/' + model_name + '/' + augs[1] + '/' + dataset + '/' + '/'\n",
    "        for file in os.listdir(PATH):\n",
    "            if file.endswith(\".pth\"):\n",
    "                model_path = os.path.join(PATH, file)\n",
    "                model_path = model_path.replace(\"//\", \"/\")\n",
    "                print(model_path)\n",
    "                if model_name == 'ResNet':\n",
    "                    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', model_num_class=2)\n",
    "                    model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "                    print(\"loaded \" + model_name + \" weights!\")\n",
    "                    print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + dataset)\n",
    "                    predictions = Video_Evaluate(model, video_test_set_paths_dictionary, dataset)\n",
    "                elif model_name == 'TimeSformer':\n",
    "                    model = TimeSformer(img_size=224, num_classes=2, num_frames=8, attention_type='divided_space_time',\n",
    "                            pretrained_model='../video_model_pretraining_weights/TimeSformer_divST_8x32_224_K400.pyth')\n",
    "                    model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "                    print(\"loaded \" + model_name + \" weights!\")\n",
    "                    print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + dataset)\n",
    "                    predictions = Video_Evaluate(model, video_test_set_paths_dictionary, dataset)\n",
    "                elif model_name == 'swin_base_patch4_window7_224':\n",
    "                    model = timm.create_model(model_name, pretrained=False, \n",
    "                            pretrained_cfg_overlay=dict(file=model_path),\n",
    "                            num_classes=2).to(device)\n",
    "                    model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "                    print(\"loaded \" + model_name + \" weights!\")\n",
    "                    print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + dataset)\n",
    "                    predictions = Evaluate(model, test_set_paths_dictionary, dataset)\n",
    "                else:\n",
    "                    model = timm.create_model(model_name, pretrained=True, num_classes=2).to(device)\n",
    "                    model.load_state_dict(torch.load(model_path), strict=True)\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "                    print(\"loaded \" + model_name + \" weights!\")\n",
    "                    print('Evaluating ' + model_name + ' on ' + dataset + ' trained on ' + dataset)\n",
    "#                     save_path = ('CrossEvaluationTestScores/Image_by_Image_Evaluation/' + model_name + '_trained_on_' + \n",
    "#                                  dataset)\n",
    "                    predictions = Evaluate(model, test_set_paths_dictionary, dataset)\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        predicted_probs = []\n",
    "        for key, value in predictions.items():\n",
    "            predicted_probs.append(value[0])\n",
    "            true_labels.append(value[2])\n",
    "            predicted_labels.append(value[1])\n",
    "        if model_name == 'xception':\n",
    "            xception_fpr, xception_tpr, _ = metrics.det_curve(true_labels, predicted_probs)\n",
    "#             xception_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'res2net101_26w_4s':\n",
    "            res2net_fpr, res2net_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             res2net_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'tf_efficientnet_b7_ns':\n",
    "            efficientnet_fpr, efficientnet_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             efficientnet_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'vit_base_patch16_224':\n",
    "            vit_fpr, vit_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             vit_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'swin_base_patch4_window7_224':\n",
    "            swin_fpr, swin_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             swin_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'mvitv2_base':\n",
    "            mvitv2_fpr, mvitv2_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             mvitv2_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'ResNet':\n",
    "            resnet_fpr, resnet_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             resnet_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "        elif model_name == 'TimeSformer':\n",
    "            timesformer_fpr, timesformer_tpr, _ = metrics.det_curve(true_labels,  predicted_probs)\n",
    "#             timesformer_auc = metrics.roc_auc_score(true_labels, predicted_probs)\n",
    "    \n",
    "    with theme:       \n",
    "        plt.plot(xception_fpr,xception_tpr,label=\"Xception\")\n",
    "        plt.plot(res2net_fpr,res2net_tpr,label=\"Res2Net\")\n",
    "        plt.plot(efficientnet_fpr,efficientnet_tpr,label=\"EfficientNet\")\n",
    "        plt.plot(vit_fpr,vit_tpr,label=\"ViT\")\n",
    "        plt.plot(swin_fpr,swin_tpr,label=\"Swin\")\n",
    "        plt.plot(mvitv2_fpr,mvitv2_tpr,label=\"MViT\")\n",
    "        plt.plot(resnet_fpr,resnet_tpr,label=\"ResNet 3D\")\n",
    "        plt.plot(timesformer_fpr,timesformer_tpr,label=\"TimeSformer\")\n",
    "        plt.legend(loc=1)\n",
    "        # plt.axis('off')\n",
    "        plt.savefig(\"NEW_\" + str(dataset) + '_' + augs[1] + \"_det.png\", bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    print('====================================================================================================')\n",
    "    print('')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013265c",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b8a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c669e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           0.9999955892562866\n",
      "1           0.9999945163726807\n",
      "2            0.999990701675415\n",
      "3           0.9999920129776001\n",
      "4           0.9999855756759644\n",
      "                 ...          \n",
      "1997    0.00014328956604003906\n",
      "1998    0.00011324882507324219\n",
      "1999    0.00010669231414794922\n",
      "2000                       AUC\n",
      "2001                  0.999963\n",
      "Name: ProbabilityScore, Length: 2002, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'CrossEvaluationTestScores/Image_by_Image_Evaluation/mvitv2_base_trained_on_CelebDFV2_evaluated_on_CelebDFV2.csv')\n",
    "print(df['ProbabilityScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9955269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xception dfdc\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "predicted_probs = []\n",
    "for key, value in predictions.items():\n",
    "    predicted_probs.append(value[0])\n",
    "    true_labels.append(value[2])\n",
    "    predicted_labels.append(value[1])\n",
    "    \n",
    "# y_pred_proba = predicted_probs\n",
    "# y_test = true_labels\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"Xception, auc=\"+str(round(auc*100, 2)))\n",
    "plt.plot(swinfpr,swintpr,label=\"Swin, auc=\"+str(round(swinauc*100, 2)))\n",
    "plt.legend(loc=4)\n",
    "# plt.axis('off')\n",
    "plt.savefig(\"roc.png\", bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea219e",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636d17ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99925"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "for key, value in predictions.items():\n",
    "    predicted_probs.append(value[0])\n",
    "    true_labels.append(value[2])\n",
    "    predicted_labels.append(value[1])\n",
    "    \n",
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18feed39",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f7c4912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2000,    0],\n",
       "       [   3, 1997]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "for key, value in predictions.items():\n",
    "    predicted_probs.append(value[0])\n",
    "    true_labels.append(value[2])\n",
    "    predicted_labels.append(value[1])\n",
    "    \n",
    "confmtrx = confusion_matrix(true_labels, predicted_labels)\n",
    "confmtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ec61263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confmtrx, annot=True, fmt='g', ax=ax, cmap='Greens');  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['fake', 'real']); ax.yaxis.set_ticklabels(['fake', 'real']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0af1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
